# Acquired Bookshelf - Production Deployment Guide

## Overview

This guide walks you through deploying the optimized RSS-based scraper system to production. The new system reduces API calls by 95% while improving reliability and speed.

## Prerequisites

- [x] Render.com account
- [x] GitHub repository with latest code
- [x] Domain configured (if using custom domain)

## Phase 1: Deploy Current System Fixes

### 1.1 Test Fixed Scraper Locally

```bash
# Test that recent fixes work
npm run scraper

# Should find 164+ episodes and handle recent episodes properly
# Look for: Google, Epic Systems, Jamie Dimon episodes
```

### 1.2 Update Render Configuration

Edit your `render.yaml` to include the new optimized scraper:

```yaml
services:
  # Existing web service (keep as-is)
  - type: web
    name: acquired-bookshelf
    env: node
    buildCommand: npm ci && npm run build
    startCommand: npm start
    
  # UPDATED: Enhanced weekly scraper (backup)
  - type: cron
    name: weekly-scraper-backup  
    env: node
    schedule: "0 10 * * 1"  # Monday 10 AM UTC (backup)
    buildCommand: npm ci
    startCommand: npm run scraper
    
  # NEW: Optimized RSS scraper (primary)
  - type: cron
    name: optimized-scraper-primary
    env: node  
    schedule: "0 */6 * * *"  # Every 6 hours
    buildCommand: npm ci
    startCommand: npm run optimized-scraper
```

### 1.3 Deploy to Render

```bash
# Commit and push changes
git add .
git commit -m "feat: Add optimized RSS-based scraper system

- RSS monitoring with 6-hour intervals
- Adaptive delay handling for blog post timing
- Smart episode classification (skip interviews)  
- Enhanced security with URL validation
- 95% reduction in unnecessary API calls"

git push origin main
```

## Phase 2: Monitor Transition

### 2.1 Verify Both Scrapers Work

**Check Render Dashboard:**
1. Go to render.com dashboard
2. Find your services
3. Check cron job logs for both scrapers

**Weekly Scraper (Backup) Logs Should Show:**
```
ðŸš€ Starting weekly scraper...
ðŸ“ Found 164 total episodes  
ðŸ†• Found 0 new episodes to process (or X new episodes)
âœ… Weekly scraper completed successfully
```

**Optimized Scraper Logs Should Show:**
```
ðŸš€ Starting optimized scraper...
ðŸ“¡ Phase 1: RSS Monitoring
ðŸ” Checking RSS feed for new episodes...
ðŸ“¡ Found X episodes in RSS feed
ðŸ†• Found X new episodes (or âœ… No new episodes found)
```

### 2.2 Test New Episode Detection

**When Acquired publishes a new episode:**

1. **RSS scraper should detect it within 6 hours**
2. **Processing flow:**
   ```
   New episode detected â†’ Try to find Google Doc immediately
   â”œâ”€ Found sources â†’ Process books â†’ âœ… Complete  
   â””â”€ No sources â†’ Requeue with 2-hour delay â†’ Retry â†’ Success/Skip
   ```

3. **Check `public/data/books.json` for new entries with:**
   ```json
   {
     "source": "automated",
     "addedAt": "2025-08-18T23:42:10.115Z"
   }
   ```

## Phase 3: Performance Monitoring

### 3.1 Key Metrics to Track

| Metric | Target | Check |
|--------|--------|-------|
| RSS Check Frequency | Every 6 hours | Render cron logs |
| New Episode Detection | Within 6 hours | Compare with podcast release |
| Processing Success Rate | >90% on first try | Book addition logs |
| API Call Reduction | 95% fewer requests | Compare weekly logs |
| Interview Episode Filtering | 100% skipped | No interview books added |

### 3.2 Success Indicators

**âœ… System is working when:**
- New episodes appear in books.json within 8 hours of podcast release
- Interview episodes are automatically skipped
- No duplicate books are created
- Cover images load reliably with fallback chain

**âŒ Investigate if:**
- No new books for >2 weeks (assuming new episodes exist)
- RSS scraper consistently failing in logs
- Duplicate books appearing
- Many books with missing covers

## Phase 4: Optimization & Scaling

### 4.1 Fine-tune Timing (Optional)

If blog posts consistently appear >2 hours after podcast release:

```typescript
// In lib/rss-monitor.ts, adjust delay:
return 4 * 60 * 60 * 1000 // 4 hours instead of 2
```

### 4.2 Add Monitoring Alerts (Recommended)

**Option 1: Simple Email Alerts**
Add to optimized scraper success/failure:

```typescript
// Send email on consistent failures
if (failureCount > 3) {
  // Send alert email
}
```

**Option 2: Render Notifications**
- Configure Render to email you on cron job failures
- Set up Slack webhooks for real-time alerts

### 4.3 Disable Old Scraper (After 2 weeks)

Once optimized scraper proves reliable:

```yaml
# Comment out or remove backup scraper
# - type: cron  
#   name: weekly-scraper-backup
#   schedule: "0 10 * * 1"
```

## Phase 5: Advanced Features (Future)

### 5.1 Enhanced Book Data

The system is ready for these additions:

- **Reading difficulty scores**
- **Book summaries from AI**
- **Related book recommendations** 
- **Price tracking from Amazon**

### 5.2 Performance Dashboards

Create monitoring dashboard:
- Episode processing times
- API success rates
- Book discovery trends
- Cover image source distribution

### 5.3 User Features

With reliable automation:
- **Email notifications** for new books
- **RSS feed** of discovered books
- **Book filtering** by episode/category
- **Reading list** functionality

## Troubleshooting

### Common Issues

**RSS Scraper Not Finding New Episodes:**
1. Check if `https://feeds.transistor.fm/acquired` is accessible
2. Verify cron job is running (check Render logs)
3. Look for network/timeout errors in logs

**Books Missing Sources/Covers:**
1. Check if episode has "Episode sources" link on acquired.fm
2. Verify Google Doc is publicly accessible
3. Test OpenLibrary API manually: `curl "https://openlibrary.org/search.json?title=BOOK_TITLE"`

**Duplicate Books:**
1. Check book ID generation (ASIN extraction)
2. Verify deduplication logic in `updateBooksDatabase()`
3. Look for URL variations in Amazon links

**Performance Issues:**
1. Monitor Render resource usage
2. Check for memory leaks in long-running processes
3. Consider adding request batching/throttling

### Rollback Plan

If optimized scraper fails:

1. **Immediate**: Weekly backup scraper continues working
2. **Quick fix**: Disable optimized scraper, increase weekly frequency:
   ```yaml
   schedule: "0 */12 * * *"  # Every 12 hours instead of weekly
   ```
3. **Debug**: Use logs to identify and fix issues in optimized scraper

### Support

- **Logs**: Render.com dashboard â†’ Services â†’ Cron Jobs â†’ Logs
- **GitHub Issues**: Document any bugs or improvements needed
- **RSS Feed**: Monitor `https://feeds.transistor.fm/acquired` manually if needed

---

## Quick Reference

### Key Commands
```bash
# Test scrapers locally
npm run scraper              # Original (backup)
npm run optimized-scraper   # New RSS-based

# Install new dependencies
npm install fast-xml-parser p-retry

# Deploy to production
git push origin main
```

### Important Files
```
scripts/optimized-scraper.ts     # Main RSS scraper
lib/rss-monitor.ts              # RSS feed monitoring  
lib/episode-classifier.ts       # Interview detection
lib/url-validator.ts           # Security validation
public/data/books.json         # Book database
render.yaml                    # Deployment config
```

### Monitoring URLs
- **Render Dashboard**: https://dashboard.render.com
- **RSS Feed**: https://feeds.transistor.fm/acquired  
- **Episode Pages**: https://www.acquired.fm/episodes/[episode-name]
- **Books Data**: https://[your-domain]/data/books.json